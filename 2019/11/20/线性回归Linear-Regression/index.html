<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>线性回归 Linear Regression | Wildness</title><meta name="description" content="线性回归 Linear Regression"><meta name="keywords" content="笔记,算法"><meta name="author" content="Yuan Ye"><meta name="copyright" content="Yuan Ye"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="canonical" href="http://yuanyenogg.github.io/2019/11/20/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92Linear-Regression/"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="线性回归 Linear Regression"><meta name="twitter:description" content="线性回归 Linear Regression"><meta name="twitter:image" content="https://i.loli.net/2019/11/20/wt8avdzIUC5iM4Z.png"><meta property="og:type" content="article"><meta property="og:title" content="线性回归 Linear Regression"><meta property="og:url" content="http://yuanyenogg.github.io/2019/11/20/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92Linear-Regression/"><meta property="og:site_name" content="Wildness"><meta property="og:description" content="线性回归 Linear Regression"><meta property="og:image" content="https://i.loli.net/2019/11/20/wt8avdzIUC5iM4Z.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="prev" title="线性分类 Linear Classification" href="http://yuanyenogg.github.io/2019/11/28/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BBLinear-Classification/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://blog.liming.ml/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: '添加书签',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天'

  
}</script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#0-概述"><span class="toc-number">1.</span> <span class="toc-text">0.概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-最小二乘估计和极大似然估计"><span class="toc-number">2.</span> <span class="toc-text">1.最小二乘估计和极大似然估计</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1最小二乘估计"><span class="toc-number">2.1.</span> <span class="toc-text">1.1最小二乘估计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2极大似然估计"><span class="toc-number">2.2.</span> <span class="toc-text">1.2极大似然估计</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3总结"><span class="toc-number">2.3.</span> <span class="toc-text">1.3总结</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-岭回归和贝叶斯角度"><span class="toc-number">3.</span> <span class="toc-text">2.岭回归和贝叶斯角度</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1正则化"><span class="toc-number">3.1.</span> <span class="toc-text">2.1正则化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2岭回归"><span class="toc-number">3.2.</span> <span class="toc-text">2.2岭回归</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3贝叶斯角度"><span class="toc-number">3.3.</span> <span class="toc-text">2.3贝叶斯角度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4总结"><span class="toc-number">3.4.</span> <span class="toc-text">2.4总结</span></a></li></ol></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://i.loli.net/2019/11/20/wt8avdzIUC5iM4Z.png)"><div id="page-header"><span class="pull-left"> <a class="blog_title" id="site-name" href="/">Wildness</a></span><div class="open toggle-menu pull-right"><div class="menu-icon-first"></div><div class="menu-icon-second"></div><div class="menu-icon-third"></div></div><span class="pull-right menus"><div class="mobile_author_icon"><img class="lozad" src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'"><div class="mobile_author-info__description"></div></div><hr><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a><script>document.body.addEventListener('touchstart', function(){ });</script></div></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title"><div class="posttitle">线性回归 Linear Regression</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2019-11-20<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> 更新于 2019-11-28</time><span class="post-meta__separator mobile_hidden">|</span><span class="mobile_hidden"><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><p><em>本文学习自<a href="https://space.bilibili.com/97068901/channel/detail?cid=92387" target="_blank" rel="noopener">bilibili机器学习白板推导系列</a>，<a href="https://songshuhui.net/archives/76501" target="_blank" rel="noopener">正态分布的前世今生</a>。</em></p>
<h2 id="0-概述"><a href="#0-概述" class="headerlink" title="0.概述"></a>0.概述</h2><p>数据集 </p>
<script type="math/tex; mode=display">D = \lbrace(x_1,y_1),(x_2,y_2),{\ldots},(x_N,y_N)\rbrace</script><p>其中，$ x_i\in\mathbb{R}^p $，$ y_i\in\mathbb{R} $，$ i=1,2,\cdots,N $<br>将数据集用矩阵形式来表示</p>
<script type="math/tex; mode=display">X = \begin{pmatrix} x_1 & x_2 & \cdots & x_N \end{pmatrix}^{T} = \begin{pmatrix} x_1^T \\ x_2^T  \\ \vdots\ \\ x_N^T \end{pmatrix} = \begin{pmatrix} x_{11} & x_{12} & \cdots & x_{1p} \\ x_{21} & x_{22} & \cdots & x_{2p} \\ \vdots & \vdots & \ddots & \vdots \\ x_{N1} & x_{N2} & \cdots & x_{Np} \end{pmatrix}_{N*p}</script><script type="math/tex; mode=display">Y = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_N \end{pmatrix}_{N*1}</script><p>我们将要拟合的函数写为 $ f(x) = w^Tx $，此处为了方便后续推导省略了偏置项 $ b $，其中<script type="math/tex">w = \begin{pmatrix} w_1 \\ w_2 \\ \vdots \\ w_p \end{pmatrix}</script>。</p>
<h2 id="1-最小二乘估计和极大似然估计"><a href="#1-最小二乘估计和极大似然估计" class="headerlink" title="1.最小二乘估计和极大似然估计"></a>1.最小二乘估计和极大似然估计</h2><h3 id="1-1最小二乘估计"><a href="#1-1最小二乘估计" class="headerlink" title="1.1最小二乘估计"></a>1.1最小二乘估计</h3><p>定义损失函数：</p>
<script type="math/tex; mode=display">
\begin{aligned}
L(w) =& \sum_{i=1}^N \| w^Tx_i-y_y \|^2 \\
=& \sum_{i=1}^N (w^Tx_i-y_i)^2 \\
=& \begin{pmatrix} w^Tx_1-y_1 & w^Tx_2-y_2 & \cdots  &w^Tx_N-y_N\end{pmatrix} \begin{pmatrix} w^Tx_1-y_1 \\ w^Tx_2-y_2 \\ \vdots \\ w^Tx_N-y_N\end{pmatrix} \\
=& (w^TX^T-Y^T) (Xw-Y) \\
=& w^TX^TXw-2w^TX^TT+Y^TY
\end{aligned}</script><p>那么，有</p>
<script type="math/tex; mode=display">\hat{w} = arg\ min\ L(w)</script><script type="math/tex; mode=display">\frac {\partial L(w)} {\partial w} = 2X^TXw-2X^TY</script><script type="math/tex; mode=display">\implies w = (X^TX)^{-1}X^TY</script><h3 id="1-2极大似然估计"><a href="#1-2极大似然估计" class="headerlink" title="1.2极大似然估计"></a>1.2极大似然估计</h3><p>从最小二乘估计我们可以知道，只有当N个样本处于同一条线上，我们拟合出来的函数最正确。但是现实生活中样本的分布是有噪声的。假设噪声 $ \varepsilon $ ~ $ N(0, \sigma^2) $ (高斯分布/正态分布，主要作用于误差分析)。<br>则可以得到：</p>
<script type="math/tex; mode=display">f(w) = w^Tx</script><script type="math/tex; mode=display">\begin{aligned} y=&f(w)+\varepsilon \\ =& w^Tx+\varepsilon \end{aligned}</script><script type="math/tex; mode=display">\implies y|x;w \sim N(w^Tx,\sigma^2)</script><script type="math/tex; mode=display">P(y|x;w) = \frac {1} {\sqrt{2\pi} \sigma} exp\{ - \frac {(y-w^Tx)^2} {2 \sigma^2} \}</script><p>即y也是服从高斯分布的，下面利用极大似然估计（MLE，log-likelihood）来对$ w $进行分析。假设样本独立同分布。<br>定义函数并用MLE分析：</p>
<script type="math/tex; mode=display">
\begin{aligned}
L(w) =& \log P(Y|X;w) \\
=& \log \prod_{i=1}^N P(y_i|x_i;w) \\
=& \sum_{i=1}^N \log P(y_i|x_i;w) \\
=& \sum \{ \log \frac {1} {\sqrt{2\pi}\sigma} - \frac {(y-w^Tx)^2} {2\sigma^2} \} 
\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}
\implies \hat{w} =& arg\ max \  L(w) \\
 =& arg \ max \  - \frac {1} {2\sigma^2} (y-w^T)^2 \\
 =& arg \ min \ (y-w^Tx)^2 \iff 最小二乘损失函数
\end{aligned}</script><h3 id="1-3总结"><a href="#1-3总结" class="headerlink" title="1.3总结"></a>1.3总结</h3><p>由此我们可以得出：</p>
<script type="math/tex; mode=display">LSE \iff MLE (noise \ is \ Gaussian \ Distribution)</script><p>即：线性回归的最小二乘估计得到的损失函数和噪声为高斯分布的极大似然估计得到的是相同的,也可以说最小二乘估计隐含着噪声是高斯分布。都能得到 $ w = (X^TX)^{-1}X^TY$，其中$ (X^TX)^{-1}X^T $ 称为$ X $的<strong>伪逆</strong>。</p>
<h2 id="2-岭回归和贝叶斯角度"><a href="#2-岭回归和贝叶斯角度" class="headerlink" title="2.岭回归和贝叶斯角度"></a>2.岭回归和贝叶斯角度</h2><h3 id="2-1正则化"><a href="#2-1正则化" class="headerlink" title="2.1正则化"></a>2.1正则化</h3><p>先写出之前的结果：</p>
<script type="math/tex; mode=display">Loss\ Function: L(w) = \sum_{i=1}^N \|w^Tx_i-y_i\|^2</script><script type="math/tex; mode=display">\hat{w} = (X^TX)^{-1}X^TY</script><p>也就是说需要上式要先求逆。数据集$X$有$N$个样本，且每个样本都是$p$维的。一般来说$N$都是远大于$p$的，但是在现实生活中可能样本的维度$p$很高，反而样本数$N$很少。<br>这导致了从数学上，可能会带来$ X^TX $的不可逆。从现象上，会导致过拟合（<em>over fitting</em>）。<br>为了解决过拟合，一般有下面几种方法:</p>
<ul>
<li>加数据</li>
<li>特征选择/特征提取（PCA）；降维。</li>
<li>正则化<br><strong>正则化框架</strong>：对目标函数加一个约束，在loss后加一个惩罚项（<em>penalty</em>）。<script type="math/tex; mode=display">arg\ min [L(w) + \lambda P(w)</script>一般我们可以引入两种具体的正则化：</li>
<li>$L1:Lasso,\ P(w) = |w|$，即一范数。</li>
<li>$L2:Ridge,\ P(w) = |w|_2^2 = w^Tw $，即二范数，岭回归。</li>
</ul>
<h3 id="2-2岭回归"><a href="#2-2岭回归" class="headerlink" title="2.2岭回归"></a>2.2岭回归</h3><p>岭回归又称<strong>权值衰减</strong>。下面利用L2正则化推导$w$：<br>定义：</p>
<script type="math/tex; mode=display">
\begin{aligned}
J(w) =& \sum_{i=1}^N ( \|w^Tx_i-y_i\|^2+\lambda w^Tw) \\
=& (w^TX^T-Y^T) (Xw-Y)+\lambda w^Tw \\
=& w^TX^TXw-2w^TX^TY+Y^TY+\lambda w^Tw \\
=& w^T(X^TX+\lambda I)w - 2w^TX^TY+Y^TY
\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}
\implies \frac {\partial{J(w)}} {\partial{w}} =& 2(X^TX+\lambda I)w -2X^TY = 0
\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}
\implies w =(X^TX+\lambda I)^{-1}X^TY
\end{aligned}</script><p>推导完毕。</p>
<h3 id="2-3贝叶斯角度"><a href="#2-3贝叶斯角度" class="headerlink" title="2.3贝叶斯角度"></a>2.3贝叶斯角度</h3><p>从贝叶斯角度，我们首先给$w$一个先验：$ w \sim N(0,\sigma_0^2)$。<br>那么，有后验$ P(w|y) = \frac {P(y|w)P(w)} {P(y)} $。<br>根据之前的1.2极大似然估计我们可以有如下世子</p>
<script type="math/tex; mode=display">P(y|w) = \frac {1} {\sqrt{2\pi}\sigma} exp \{ - \frac {(y-w^Tx)^2} {2\sigma^2}\}</script><script type="math/tex; mode=display">P(w) = \frac {1} {\sqrt{2\pi}\sigma_0} exp \{ - \frac {\|w\|^2} {2\sigma_0^2} \}</script><p>利用极大化后验估计来估计参数$w$：</p>
<script type="math/tex; mode=display">
\begin{aligned}
MAP: \hat{w} = & arg\ max\ P(w|y) \\
=& arg\ max\ P(y|w)P(w) \\
=& arg\ max\ \log [P(y|w)P(w)] \\
=& arg\ max\ - \frac {(y-w^Tx)^2} {2\sigma^2} - \frac {\|w\|^2} {2\sigma_0^2} \\
=& arg\ min\ (y-w^Tx)^2+ \frac {\sigma^2} {\sigma_0^2} \|w\|^2 \\
\iff Ridge\ Regression\ Loss\ Function
\end{aligned}</script><h3 id="2-4总结"><a href="#2-4总结" class="headerlink" title="2.4总结"></a>2.4总结</h3><p>由此我们可以得出：</p>
<script type="math/tex; mode=display">Regularized\ LSE \iff MAP (noise \ is \ GD，prior\ is\ GD)</script><p>即：L2正则化的最小二乘估计的损失函数和噪声为高斯分布的极大后验估计是一样的。</p>
<p>完。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined" target="_blank" rel="noopener">Yuan Ye</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://yuanyenogg.github.io/2019/11/20/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92Linear-Regression/">http://yuanyenogg.github.io/2019/11/20/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92Linear-Regression/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yuanyenogg.github.io">Wildness</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%AC%94%E8%AE%B0/">笔记    </a><a class="post-meta__tags" href="/tags/%E7%AE%97%E6%B3%95/">算法    </a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2019/11/20/wt8avdzIUC5iM4Z.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><nav class="pagination_post" id="pagination"><div class="prev-post pull-full"><a href="/2019/11/28/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BBLinear-Classification/"><img class="prev_cover lozad" data-src="https://i.loli.net/2019/11/28/d9fj75LGWyKelOg.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>线性分类 Linear Classification</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2019/11/28/线性分类Linear-Classification/" title="线性分类 Linear Classification"><img class="relatedPosts_cover lozad"data-src="https://i.loli.net/2019/11/28/d9fj75LGWyKelOg.png"><div class="relatedPosts_title">线性分类 Linear Classification</div></a></div><div class="relatedPosts_item"><a href="/2019/11/28/深入理解计算机系统/" title="深入理解计算机系统"><img class="relatedPosts_cover lozad"data-src="https://i.loli.net/2019/11/28/RjFSQsG1rkV6eDo.png"><div class="relatedPosts_title">深入理解计算机系统</div></a></div></div><div class="clear_both"></div></div></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2019 By Yuan Ye</div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><section class="rightside" id="rightside"><i class="fa fa-book" id="readmode" title="阅读模式"> </i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" target="_blank" rel="noopener" title="简繁转换">简</a><i class="fa fa-moon-o nightshift" id="nightshift" title="夜间模式"></i></section><div id="post_bottom"><div id="post_bottom_items"><a id="to_comment" href="#post-comment"><i class="scroll_to_comment fa fa-comments"></i></a><i class="fa fa-list" id="mobile_toc"></i><div id="toc_mobile"><div class="toc_mobile_headline">目录</div><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#0-概述"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">0.概述</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#1-最小二乘估计和极大似然估计"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">1.最小二乘估计和极大似然估计</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#1-1最小二乘估计"><span class="toc_mobile_items-number">2.1.</span> <span class="toc_mobile_items-text">1.1最小二乘估计</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#1-2极大似然估计"><span class="toc_mobile_items-number">2.2.</span> <span class="toc_mobile_items-text">1.2极大似然估计</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#1-3总结"><span class="toc_mobile_items-number">2.3.</span> <span class="toc_mobile_items-text">1.3总结</span></a></li></ol></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#2-岭回归和贝叶斯角度"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">2.岭回归和贝叶斯角度</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2-1正则化"><span class="toc_mobile_items-number">3.1.</span> <span class="toc_mobile_items-text">2.1正则化</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2-2岭回归"><span class="toc_mobile_items-number">3.2.</span> <span class="toc_mobile_items-text">2.2岭回归</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2-3贝叶斯角度"><span class="toc_mobile_items-number">3.3.</span> <span class="toc_mobile_items-text">2.3贝叶斯角度</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2-4总结"><span class="toc_mobile_items-number">3.4.</span> <span class="toc_mobile_items-text">2.4总结</span></a></li></ol></li></ol></div></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/js/nightshift.js"></script><script src="/js/tw_cn.js"></script><script>translateInitilization()

</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@1.2.2/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script><script>const observer = lozad(); // lazy loads elements with default selector as '.lozad'
observer.observe();</script></body></html>