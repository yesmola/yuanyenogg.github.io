<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>线性分类 Linear Classification | Wildness</title><meta name="description" content="线性分类 Linear Classification"><meta name="keywords" content="笔记,算法"><meta name="author" content="Yuan Ye"><meta name="copyright" content="Yuan Ye"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="canonical" href="http://yuanyenogg.github.io/2019/11/28/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BBLinear-Classification/"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="线性分类 Linear Classification"><meta name="twitter:description" content="线性分类 Linear Classification"><meta name="twitter:image" content="https://i.loli.net/2019/11/28/d9fj75LGWyKelOg.png"><meta property="og:type" content="article"><meta property="og:title" content="线性分类 Linear Classification"><meta property="og:url" content="http://yuanyenogg.github.io/2019/11/28/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BBLinear-Classification/"><meta property="og:site_name" content="Wildness"><meta property="og:description" content="线性分类 Linear Classification"><meta property="og:image" content="https://i.loli.net/2019/11/28/d9fj75LGWyKelOg.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="prev" title="降维 Dimensionality Reduction" href="http://yuanyenogg.github.io/2019/12/03/%E9%99%8D%E7%BB%B4/"><link rel="next" title="深入理解计算机系统" href="http://yuanyenogg.github.io/2019/11/27/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://blog.liming.ml/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: '添加书签',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天'

  
}</script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#0-频率派和贝叶斯派"><span class="toc-number">1.</span> <span class="toc-text">0.频率派和贝叶斯派</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-感知机算法-Perceptron"><span class="toc-number">2.</span> <span class="toc-text">1.感知机算法 Perceptron</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-线性判别分析-Linear-Discriminant-Analysis-LDA-fisher"><span class="toc-number">3.</span> <span class="toc-text">2.线性判别分析(Linear Discriminant Analysis,LDA) fisher</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-逻辑回归-Logistic-Regression-LR"><span class="toc-number">4.</span> <span class="toc-text">3.逻辑回归 Logistic Regression,LR</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-高斯判别分析-Gaussian-Discriminant-Analysis-GDA"><span class="toc-number">5.</span> <span class="toc-text">4.高斯判别分析 Gaussian Discriminant Analysis,GDA</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-朴素贝叶斯分类-Naive-Bayes-Classifier"><span class="toc-number">6.</span> <span class="toc-text">5.朴素贝叶斯分类 Naive Bayes Classifier</span></a></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://i.loli.net/2019/11/28/d9fj75LGWyKelOg.png)"><div id="page-header"><span class="pull-left"> <a class="blog_title" id="site-name" href="/">Wildness</a></span><div class="open toggle-menu pull-right"><div class="menu-icon-first"></div><div class="menu-icon-second"></div><div class="menu-icon-third"></div></div><span class="pull-right menus"><div class="mobile_author_icon"><img class="lozad" src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'"><div class="mobile_author-info__description"></div></div><hr><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a><script>document.body.addEventListener('touchstart', function(){ });</script></div></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title"><div class="posttitle">线性分类 Linear Classification</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2019-11-28<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> 更新于 2019-12-04</time><span class="post-meta__separator mobile_hidden">|</span><span class="mobile_hidden"><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><p><em>本文学习自<a href="https://space.bilibili.com/97068901/channel/detail?cid=92387" target="_blank" rel="noopener">bilibili机器学习白板推导系列</a>，<a href="https://www.cnblogs.com/pinard/p/6244265.html" target="_blank" rel="noopener">线性判别分析LDA原理总结</a>，部分图片来源于网络。</em></p>
<h2 id="0-频率派和贝叶斯派"><a href="#0-频率派和贝叶斯派" class="headerlink" title="0.频率派和贝叶斯派"></a>0.频率派和贝叶斯派</h2><p>一般来说机器学习可以分为两种门派，频率派（Frequentis）和贝叶斯派(Bayesian)。现代机器学习都是一个对目标函数的优化问题，对应于频率派使用极大似然估计（Maximum Likelihood Estimation,MLE），贝叶斯派使用极大后验估计（Maximum A Posteriori,MAP）。频率派认为参数是固定的，目标找到这个参数的值。贝叶斯派认为参数是满足一个分布的，目标找到最优的对参数的分布描述。<em><a href="https://www.zhihu.com/question/20587681" target="_blank" rel="noopener">贝叶斯派和频率派有何不同</a></em></p>
<ul>
<li>贝叶斯派 $ \to $ 概率图模型 </li>
<li>频率派 $ \to $ 统计机器学习<ul>
<li>线性回归是统计机器学习中一个最基础、最简单也是最核心的模型，打破下面的三个性质中的某个性质可以带来新的模型。<ul>
<li>线性 <script type="math/tex">\to \begin{cases} \text{属性非线性：特征转换（多项式回归）} \\ \text{全局非线性：线性分类} \\ \text{系数非线性：神经网络} \end{cases}</script></li>
<li>全局性 $ \to \text{线性样条回归、决策树}$</li>
<li>数据未加工 $ \to \text {PCA，流形} $</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>$ \require{AMScd} $</p>
<script type="math/tex; mode=display">
\begin{CD}
\text{线性回归} @>\text{激活函数(activation function)）&降维}>> 线性分类
\end{CD}</script><p>同时我们将线性分类做一个划分：</p>
<script type="math/tex; mode=display">
\text{线性分类}
\begin{cases}
软分类 \begin{cases} \text{线性判别分析：fisher} \\ \text{感知机} \end{cases} \\
硬分类 \begin{cases} \text{生成式：高斯判别分析Gaussian Discriminant Analysis/朴素贝叶斯Naive Bayes} \\ \text{判别式：逻辑回归Logistic Regression} \end{cases}
\end{cases}</script><h2 id="1-感知机算法-Perceptron"><a href="#1-感知机算法-Perceptron" class="headerlink" title="1.感知机算法 Perceptron"></a>1.感知机算法 Perceptron</h2><p><strong>思想：错误驱动</strong><br>模型：$ f(x) = sign(w^Tx), x \in \mathbb{R}^p, w\in \mathbb{R}^p, \text{省略了b}$<br>其中，符号函数<script type="math/tex">sign(a) = \begin{cases} +1,a \geq  0 \\ -1,a<0 \\ \end{cases}</script><br>策略/损失函数设计：根据错误驱动的思想，我们用一个集合$D={\text{被错误分类的样本}}$。这样我们可以设计出损失函数：</p>
<script type="math/tex; mode=display">
\begin{aligned}
L(x) =& \sum_{i=1}^NI\{y_i(w^Tx_i)\},\text{即错误样本的个数，但是不可导} \\ \Downarrow \\
L(x) =& \sum_{x \in D} -y_i(w^Tx_i)\implies\nabla_wL=-y_ix_i
\end{aligned}</script><p>算法：随机梯度下降（Stochastic Gradient Descent，SGD）</p>
<script type="math/tex; mode=display">w^{t+1} \leftarrow w^t-\lambda\nabla_wL = w^t+\lambda y_ix_i</script><p>但是感知机算法是建立在样本线性可分的基础上的，如果样本不满足线性可分，有另一个允许一点点错误的算法$pocket\,algorithm$。</p>
<h2 id="2-线性判别分析-Linear-Discriminant-Analysis-LDA-fisher"><a href="#2-线性判别分析-Linear-Discriminant-Analysis-LDA-fisher" class="headerlink" title="2.线性判别分析(Linear Discriminant Analysis,LDA) fisher"></a>2.线性判别分析(Linear Discriminant Analysis,LDA) fisher</h2><p><strong>分类问题数据表示：</strong></p>
<script type="math/tex; mode=display">X = \begin{pmatrix}x_1&x_2&\cdots&x_N)\end{pmatrix}^T = \begin{pmatrix} x_1^T \\ x_2^T \\ \vdots \\ x_N^T \end{pmatrix}_{N*p}</script><script type="math/tex; mode=display">Y = \begin{pmatrix} y_1 \\ y_2 \\ \vdots \\ y_N \end{pmatrix}_{N*1}</script><script type="math/tex; mode=display">{(x_i,y_i)}_{i=1}^N,x_i\in\mathbb{R}^p,y_i\in \lbrace+1,-1\rbrace</script><script type="math/tex; mode=display">x_{c1} = \lbrace x_i | y_i=+1 \rbrace,x_{c2} = \lbrace x_i | y_i=-1 \rbrace</script><script type="math/tex; mode=display">|x_{c1}|=N_1,|x_{c2}=N_2|,N_1+N_2=N</script><p><strong>思想：类内小，类间大。（松耦合，高内聚）</strong></p>
<p><img alt="LDA" data-src="https://i.loli.net/2019/11/29/n54ZeyFP9VITxlc.png" class="lozad"></p>
<p>线性判别分析采用了一种降维的方法，在模式识别领域有非常广泛的应用。</p>
<p>我们如果假设分类的超平面为$ y=w^Tb $ ，且$|w|=1$（因为如果不固定它的范数可以自由缩放），那么投影方向也就是该直线的法向量$\vec{w}$，那么对于任意一个样本点$ x_i $，根据向量内积的定义在平面上的投影为 $x_i$ 和 $w$的内积，即$w^Tx$用$z_i$来表示。</p>
<p>下面我们分析$z_i$的均值和方差。</p>
<script type="math/tex; mode=display">\text{均值:}\overline{z}= \frac {1} {N} \sum_{i=1}^N z_i =\frac {1} {N} \sum_{i=1}^N w^Tx_i</script><script type="math/tex; mode=display">\text{方差:}S_z= \frac {1} {N} \sum_{i=1}^N (z_i-\overline{z})(z_i-\overline{z})^T =\frac {1} {N} \sum_{i=1}^N (w^Tx_i-\overline{z})(w^Tx_i-\overline{z})^T</script><p>那么，对于类别1和类别2分别来说:</p>
<script type="math/tex; mode=display">C1: \overline{z_1}=\frac {1} {N_1} \sum_{i=1}^{N_1} w^Tx_i,S_1 =\frac {1} {N_1} \sum_{i=1}^{N_1} (w^Tx_i-\overline{z_1})(w^Tx_i-\overline{z_1})^T</script><script type="math/tex; mode=display">C2: \overline{z_2}=\frac {1} {N_2} \sum_{i=1}^{N_2} w^Tx_i,S_2 =\frac {1} {N_2} \sum_{i=1}^{N_2} (w^Tx_i-\overline{z_2})(w^Tx_i-\overline{z_2})^T</script><p><strong>所以可以用下面的方法来表示类内和类间：</strong></p>
<script type="math/tex; mode=display">类间:(\overline{z_1}-\overline{z_2})^2;类内: S_1+S_2</script><p><strong>那么根据类内小、类间大的思想，可以得到目标函数：</strong></p>
<script type="math/tex; mode=display">J(w) = \frac {(\overline{z_1}-\overline{z_2})^2} {S_1+S_2}</script><script type="math/tex; mode=display">\hat{w} = arg\ max\ J(w)</script><p>分别分析目标函数的分子和分母：</p>
<script type="math/tex; mode=display">分子=(\frac {1} {N_1} \sum_{i=1}^{N_1} w^Tx_i - \frac {1} {N_2} \sum_{i=1}^{N_2} w^Tx_i)^2 = [w^T(\overline{x_{c_1}}-\overline{x_{c_2}})]^2</script><script type="math/tex; mode=display">\begin{aligned} S_1 =& \frac {1} {N_1} \sum_{i=1}^{N_1} (w^Tx_i-\frac {1} {N_1} \sum_{j=1}^{N_1} w^Tx_j)(w^Tx_i-\frac {1} {N_1} \sum_{j=1}^{N_1} w^Tx_j)^T \\ =& \frac{1}{N_1}\sum_{i=1}^{N_1}[w^T(x_i-x_{c_1})(x_i-x_{c_1})^Tw]\\ = & w^TS_{c_1}w   \\
\implies 分母=& w^TS_{c_1}w+w^TS_{c_2}w\end{aligned}</script><p>由此：</p>
<script type="math/tex; mode=display">\begin{aligned} J(w) =& \frac {w^T(\overline{x_{c1}}-\overline{x_{c2}})(\overline{x_{c1}}-\overline{x_{c2}})^Tw} {w^T(S_{c_1}+S_{c_2})w} \\ =& \frac {w^TS_bw} {w^TS_ww} \\ =& (w^TS_bw)(w^TS_ww)^{-1}\end{aligned}</script><p>其中，$Sb$表示类间方差(<em>between-class</em>)，$Sw$表示类内方差(<em>within-class</em>)。<br>接着求$w$:</p>
<script type="math/tex; mode=display">
\begin{aligned}
&\frac {\partial{J(w)}} {\partial{w}} = 2S_bw(w^TS_ww)^{-1}+w^TS_bw(-1)(w^TS_ww)^{-2}2S_ww=0 \\
&\implies S_bw(w^TS_ww) - w^TS_bwS_ww=0 \\
&\implies \underbrace{w^TS_bw}_{\in \mathbb{R}} S_ww = S_bw \underbrace{w^TS_ww}_{\in \mathbb{R}} \\
&\implies w = \frac {w^TS_ww} {w^TS_bw} S_w^{-1}S_bw \\
&\text{只考虑方向并且展开}Sb \implies w 相关于 S_w^{-1}(\overline{x_{c_1}} - \overline{x_{c_2}})
\end{aligned}</script><p>在实际应用中，已经很少用线性判别分析做分类任务了。但是这是一个比较早的分类方法。</p>
<h2 id="3-逻辑回归-Logistic-Regression-LR"><a href="#3-逻辑回归-Logistic-Regression-LR" class="headerlink" title="3.逻辑回归 Logistic Regression,LR"></a>3.逻辑回归 Logistic Regression,LR</h2><p>数据集表示：</p>
<script type="math/tex; mode=display">\text{Data:} \lbrace (x_i,y_i) \rbrace_{i=1}^M,x_i \in \mathbb{R}^p,y_i\in \{0,1\}</script><p>逻辑回归使用了$sigmoid$函数作为激活函数将预测值映射为$(0,1)$上的概率值来帮助处理分类问题。下面可以看一下$sigmoid: \sigma(z)= \frac {1} {1+e^{-z}}$的函数图像：<br><img alt="Sigmoid" data-src="https://i.loli.net/2019/11/29/GsYKAjFndpxDkf2.png" class="lozad"></p>
<p>利用此激活函数我们可以得到：</p>
<script type="math/tex; mode=display">
\begin{cases}
p_1=P(y=1|x) = \sigma(w^Tx) = \frac {1} {1+e^{-w^Tx}},y=1 \\
p_0=P(y=0|x) = 1-P(y=1|x) = \frac {e^{-w^Tx}} {1+e^{-w^Tx}},y=0
\end{cases}</script><script type="math/tex; mode=display">\implies P(y|x) =p_1^yp_0^{1-y}</script><p>逻辑回归是一种概率判别模型，我们可以利用极大似然估计来估计出$w$的取值。我们假设样本满足独立同分布。则：</p>
<script type="math/tex; mode=display">
\begin{aligned}
MLE: \hat{w} =& arg\ max\ \log P(Y|X) \\
=& arg\ max\ \log \prod_{i=1}^{N}P(y_i|x_i) \\
=& arg\ max\ \sum_{i=1}^N \log P(y_i|x_i) \\
=& arg\ max\ \sum_{i=1}^N(y_i\log{p_1}+(1-y_i)\log{p_0}) \\
=& arg\ max\ \sum_{i=1}^N[y_i\log{\psi(x_i;w)}+(1-y_i)\log{(1-\psi(x_i;w))}]
\end{aligned}</script><p>同时我们可以得到一个概念，<strong>交叉熵(cross Emtropy)</strong>，即上式右端方括号内表达式。<br>我们知道MLE是一个最大化问题，为了利用梯度下降的方法来对该问题求解，我们也可以顺势得到逻辑回归的损失函数，负的交叉熵并进行缩放。</p>
<script type="math/tex; mode=display">J(w) = - \frac{1}{N} \sum_{i=1}^N[y_i\log{\psi(x_i;w)}+(1-y_i)\log{(1-\psi(x_i;w))}]</script><p>接着就可以用梯度下降的方式，通过对损失函数求偏导来求$w$的更新。</p>
<p>根据链式求导法则，研究某一维特征，这样就不要用矩阵求导，把参数都当一维来处理，先证明$sigmoid$函数的导数性质：</p>
<script type="math/tex; mode=display">
\begin{aligned}
(\frac {1} {1+e^{-z}})'=& ((1+e^{-z})^{-1})'\\
=&(-1)(1+e^{-z})^{-2}e^{-z}(-1) \\
=&\frac {e^{-z}} {(1+e^{-z})^2} \\
\implies \sigma'(z)=&\sigma(z)(1-\sigma(z))
\end{aligned}</script><p>接下来证明：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac {\partial{J(w)}} {\partial{w}} = &\frac {\partial{J(\psi(z_i))}} {\partial{\psi(z_i)}} \frac {\partial{\psi(z_i)}} {\partial{z_i}} \frac {\partial{z_i}} {\partial{w}} \\
=& - \frac {1}{N}\sum_{i=1}^N(y_i \frac {1} {\psi(z_i)} -(1-y_i) \frac {1} {1-\psi(z_i)}) \frac {\partial{\psi(z_i)}} {\partial{z_i}} \frac {\partial{z_i}} {\partial{w}} \\
=& - \frac {1}{N}\sum_{i=1}^N(y_i \frac {1} {\psi(z_i)} -(1-y_i) \frac {1} {1-\psi(z_i)}) (\psi(z_i)(1-\psi(zi))) x_i \\
=& - \frac {1}{N}\sum_{i=1}^N(y_i（1-\psi(z_i))-(1-y_i)\psi(z_i))x_i \\
=& - \frac {1}{N}\sum_{i=1}^N(y_i-\psi(z_i))x_i
\end{aligned}</script><p>由此可以得到权重的更新：</p>
<script type="math/tex; mode=display">w^{t+1} \leftarrow w^t + \lambda \sum_{i=1}^N(y_i-\psi(z_i))x_i</script><p>逻辑回归的推导完毕。</p>
<h2 id="4-高斯判别分析-Gaussian-Discriminant-Analysis-GDA"><a href="#4-高斯判别分析-Gaussian-Discriminant-Analysis-GDA" class="headerlink" title="4.高斯判别分析 Gaussian Discriminant Analysis,GDA"></a>4.高斯判别分析 Gaussian Discriminant Analysis,GDA</h2><p>数据集表示：</p>
<script type="math/tex; mode=display">\text{Data:} \lbrace (x_i,y_i) \rbrace_{i=1}^M,x_i \in \mathbb{R}^p,y_i\in \{0,1\}</script><p>生成模型不同于判别模型是去直接求概率值，而是利用贝叶斯定理来对联合概率建模。<br>后验$ P(y|x)，posterior$正比于似然乘以先验$ P(x|y),likelihood*P(y),prior $</p>
<script type="math/tex; mode=display">\hat{y} = arg\ max\ P(y|x) =arg\ max\ P(y)P(x|y)</script><p>我们知道$y\in {0,1}$，所以可以看作参数$y$满足伯努利分布，即$y\sim Bernoullli(\phi)$，$y=1$是概率为$\phi$。<br>而在高斯判别分析模型中，我们有如下假设：</p>
<script type="math/tex; mode=display">
\begin{aligned}
x|y=&0 \sim N(\mu_1,\Sigma) \\
x|y=&1 \sim N(\mu_2,\Sigma) \\
\implies x|y & \sim N(\mu_1,\Sigma)^yN(\mu_2,\Sigma)^{1-y}
\end{aligned}</script><p>写出联合概率的log似然函数，最大化后验估计，</p>
<script type="math/tex; mode=display">
\begin{aligned}
log-likelihood： l(\theta) =& \log \prod_{i=1}^N P(x_i,yi) \\
=& \sum_{i=1}^N \log (P(x_i|y_i)P(y_i)) \\
=& \sum_{i=1}^N [\log P(x_i|y_i)+\log P(y_i)] \\
=& \sum_{i=1}^N [\log N(\mu_1,\Sigma)^{y_i} + \log N(\mu_2,\Sigma)^{1-y_i}+log(\phi^{y_1}\phi^{1-y_i})] \\
\hat{\theta} =& arg\ max\ l(\theta)   
\end{aligned}</script><p>可以看到上式目标函数右端由三部分（A，B，C）组成，假设样本中$ y=1 $ 的样本有$ N_1 $个，$ y=0 $ 的样本有$ N_2 $个。$\theta$是与$(\mu_1,\mu_2,\Sigma,\phi)$相关的参数，下面求$\hat{\theta}$。<br>第一步，求$\phi$：</p>
<script type="math/tex; mode=display">
\begin{aligned}
C =& \sum_{i=1}^N [ y_i \log \phi + (1-y_i) \log (1-\phi)] \\
\frac {\partial{C}} {\partial \phi} =& \sum_{i=1}^N y_i \frac {1} {\phi} +(1-y_i) \frac {1} {1-\phi}(-1) \\
=& \sum_{i=1}^N y_i \frac {1} {\phi} -(1-y_i) \frac {1} {1-\phi} \\
=& 0 \\
\implies & \sum_{i=1}^N [y_i(1-\phi) - (1-y_i)\phi]=0 \\
& \sum_{i=1}^N (y_i-\phi) =0 \\
& \sum_{i=1}^N y_i - N\phi =0 \\
\implies &\hat{\phi} = \frac {1} {N} \sum_{i=1}^Ny_i= \frac {N_1}{N}
\end{aligned}</script><p>第二步，求$\mu_1$：</p>
<script type="math/tex; mode=display">
\begin{aligned}
A = & \sum_{i=1}^N \log N(\mu_1,\Sigma)^{y_i} \\
=& \sum_{i=1}^N \log \frac{1}{(2\pi)^{\frac{p} {2}} |\Sigma|^{\frac {1} {2}}} exp (- \frac {1} {2} (x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)) \\
\implies & \mu_1 = arg\ max\ A = arg\ max\ \sum_{i=1}^N y_i(-(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)) \\
& =arg\ max\ \Delta \\
\frac {\partial{\Delta}} {\partial {\mu_1}} =& \sum_{i=1}^N y_i(-2\Sigma^{-1}(x_i-\mu_1)(-1)) = 0 \\
\implies & \sum_{i=1}^N y_i\mu_1 = \sum_{i=1}^N y_i x_i \\
\implies & \hat{\mu_1} = \frac {\sum_{i=1}^N y_i x_i } {\sum_{i=1}^N y_i x_i } = \frac {\sum_{i=1}^N y_i x_i} {N_1}
\end{aligned}</script><p>$\mu_2$与$\mu_1$的求法相同，略。<br>第三部，求$\Sigma$：</p>
<script type="math/tex; mode=display">
\begin{aligned}
A+B = & \sum_{x_i \in C_1} \log N(\mu_1,\Sigma) + \sum_{x_i \in C_2} \log N(\mu_2,\Sigma) \\
\sum_{i=1}^N \log N(\mu,\Sigma) =& \sum_{i=1}^N \log \frac {1} {(2\pi)^{\frac {p} {2}}|\Sigma|^{\frac {1} {2}}}exp[-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)] \\
=&\sum_{i=1}^N \{\log \frac {1} {(2\pi)^{\frac {p} {2}}} + \log {\Sigma^{-\frac {1} {2}}}+[-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)]\} \\
=& C - \frac {1} {2} N \log |\Sigma|-\frac {1}{2} \sum_{i=1} ^N (x_i-\mu)^T\Sigma^{-1}(x_i-\mu) 
\end{aligned}</script><p>这里利用了上式右端最后一个部分矩阵相乘得到了一个实数，且实数的trace等于其本身，和一些矩阵trace的性质。假设样本分布的种类$C<em>1$和$C_2$对应的方差为$S_1$和$S_2$（$S = \frac {1} {N}\sum</em>{i=1}^N (x_i-\mu)(x_i-\mu)^T$），可以得到：</p>
<script type="math/tex; mode=display">
\begin{aligned}
A+B =& -\frac {1}{2} N_1\log|\Sigma|-\frac {1}{2} N_1tr(S_1\Sigma^{-1}) -\frac {1}{2} N_2\log|\Sigma|-\frac {1}{2} N_2tr(S_2\Sigma^{-1})+C \\
= & -\frac {1}{2}(N\log|\Sigma|+N_1tr(S_1\Sigma^{-1})+N_2tr(S_2\Sigma^{-1}))+C
\end{aligned}</script><p>利用方差矩阵是对称的和矩阵求导的一些性质$\frac {\partial(\Sigma A)} {\partial(\Sigma)} = A^T,\frac {\partial(|A|)} {\partial(A)} = |A|A^{-1} $：</p>
<script type="math/tex; mode=display">
\begin{aligned}
\frac {\partial{(A+B)}} {\partial{\Sigma}} = -\frac {1} {2}(N\Sigma^{-1}-N_1S_1\Sigma^{-2}-N_2S_2\Sigma^{-2}) =& 0 \\
N\Sigma-N_1S_1-N_2S_2=&0 \\
\implies \hat {\Sigma}=  \frac {N_1S_1+N_2S_2} {N} &
\end{aligned}</script><p>这样我们就估计出了所有的参数，通过上述公式，之后我们判断一个新样本时，可以分别利用贝叶斯求出每个类别的概率，然后取概率更大的那个类即可。</p>
<h2 id="5-朴素贝叶斯分类-Naive-Bayes-Classifier"><a href="#5-朴素贝叶斯分类-Naive-Bayes-Classifier" class="headerlink" title="5.朴素贝叶斯分类 Naive Bayes Classifier"></a>5.朴素贝叶斯分类 Naive Bayes Classifier</h2><p>数据集表示：</p>
<script type="math/tex; mode=display">\text{Data:} \lbrace (x_i,y_i) \rbrace_{i=1}^M,x_i \in \mathbb{R}^p,y_i\in \{0,1\}</script><p>思想：朴素贝叶斯假设（条件独立性假设），即$x$的$p$维属性之间是独立的。也即：</p>
<script type="math/tex; mode=display">x_i\bot x_j |y (i \not= j),P(x|y) = \prod_{j=1}^p P(x_j|y)</script><p>这样做的动机是简化运算。朴素贝叶斯是最简单的概率图模型，而且是有向图。<br>模型：</p>
<script type="math/tex; mode=display">\hat{y} = arg\ max\ P(y|x) = arg\ \max_{y=\{0,1\}} \frac {P(x,y)} {P(x)} = arg\ \max_{y} P(y)P(x|y)</script><p>对于$P(y)$：当处理二分类问题时，可以认为$ y\sim Bernoulli\ Dist $；当处理多分类问题时，可以认为$ y\sim Categorial\ Dist $。<br>对于$P(x_j|y)$：当$x$离散时，可以认为$x_j\sim Categorical\ Dist$；当$x$连续时，可以认为$x_j\sim N(\mu_j,\sigma_j^2)$。<br>我们知道了这些参数的分布后，就可以利用$MLE$来估计了。</p>
<p>至此，线性分类完毕。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined" target="_blank" rel="noopener">Yuan Ye</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://yuanyenogg.github.io/2019/11/28/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BBLinear-Classification/">http://yuanyenogg.github.io/2019/11/28/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BBLinear-Classification/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yuanyenogg.github.io">Wildness</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%AC%94%E8%AE%B0/">笔记    </a><a class="post-meta__tags" href="/tags/%E7%AE%97%E6%B3%95/">算法    </a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2019/11/28/d9fj75LGWyKelOg.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><nav class="pagination_post" id="pagination"><div class="prev-post pull-left"><a href="/2019/12/03/%E9%99%8D%E7%BB%B4/"><img class="prev_cover lozad" data-src="https://i.loli.net/2019/12/03/PhIYejAgvFkJp5d.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>降维 Dimensionality Reduction</span></div></a></div><div class="next-post pull-right"><a href="/2019/11/27/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F/"><img class="next_cover lozad" data-src="https://i.loli.net/2019/11/28/RjFSQsG1rkV6eDo.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>深入理解计算机系统</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2019/12/03/降维/" title="降维 Dimensionality Reduction"><img class="relatedPosts_cover lozad"data-src="https://i.loli.net/2019/12/03/PhIYejAgvFkJp5d.png"><div class="relatedPosts_title">降维 Dimensionality Reduction</div></a></div><div class="relatedPosts_item"><a href="/2019/11/20/线性回归Linear-Regression/" title="线性回归 Linear Regression"><img class="relatedPosts_cover lozad"data-src="https://i.loli.net/2019/11/20/wt8avdzIUC5iM4Z.png"><div class="relatedPosts_title">线性回归 Linear Regression</div></a></div><div class="relatedPosts_item"><a href="/2019/11/27/深入理解计算机系统/" title="深入理解计算机系统"><img class="relatedPosts_cover lozad"data-src="https://i.loli.net/2019/11/28/RjFSQsG1rkV6eDo.png"><div class="relatedPosts_title">深入理解计算机系统</div></a></div></div><div class="clear_both"></div></div></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2019 By Yuan Ye</div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><section class="rightside" id="rightside"><i class="fa fa-book" id="readmode" title="阅读模式"> </i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" target="_blank" rel="noopener" title="简繁转换">简</a><i class="fa fa-moon-o nightshift" id="nightshift" title="夜间模式"></i></section><div id="post_bottom"><div id="post_bottom_items"><a id="to_comment" href="#post-comment"><i class="scroll_to_comment fa fa-comments"></i></a><i class="fa fa-list" id="mobile_toc"></i><div id="toc_mobile"><div class="toc_mobile_headline">目录</div><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#0-频率派和贝叶斯派"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">0.频率派和贝叶斯派</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#1-感知机算法-Perceptron"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">1.感知机算法 Perceptron</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#2-线性判别分析-Linear-Discriminant-Analysis-LDA-fisher"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">2.线性判别分析(Linear Discriminant Analysis,LDA) fisher</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#3-逻辑回归-Logistic-Regression-LR"><span class="toc_mobile_items-number">4.</span> <span class="toc_mobile_items-text">3.逻辑回归 Logistic Regression,LR</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#4-高斯判别分析-Gaussian-Discriminant-Analysis-GDA"><span class="toc_mobile_items-number">5.</span> <span class="toc_mobile_items-text">4.高斯判别分析 Gaussian Discriminant Analysis,GDA</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#5-朴素贝叶斯分类-Naive-Bayes-Classifier"><span class="toc_mobile_items-number">6.</span> <span class="toc_mobile_items-text">5.朴素贝叶斯分类 Naive Bayes Classifier</span></a></li></ol></div></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/js/nightshift.js"></script><script src="/js/tw_cn.js"></script><script>translateInitilization()

</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@1.2.2/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script><script>const observer = lozad(); // lazy loads elements with default selector as '.lozad'
observer.observe();</script></body></html>