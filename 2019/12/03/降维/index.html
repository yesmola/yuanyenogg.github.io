<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>降维 Dimensionality Reduction | Wildness</title><meta name="description" content="降维 Dimensionality Reduction"><meta name="keywords" content="笔记,算法"><meta name="author" content="Yuan Ye"><meta name="copyright" content="Yuan Ye"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.ico"><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="canonical" href="http://yuanyenogg.github.io/2019/12/03/%E9%99%8D%E7%BB%B4/"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:title" content="降维 Dimensionality Reduction"><meta name="twitter:description" content="降维 Dimensionality Reduction"><meta name="twitter:image" content="https://i.loli.net/2019/12/03/PhIYejAgvFkJp5d.png"><meta property="og:type" content="article"><meta property="og:title" content="降维 Dimensionality Reduction"><meta property="og:url" content="http://yuanyenogg.github.io/2019/12/03/%E9%99%8D%E7%BB%B4/"><meta property="og:site_name" content="Wildness"><meta property="og:description" content="降维 Dimensionality Reduction"><meta property="og:image" content="https://i.loli.net/2019/12/03/PhIYejAgvFkJp5d.png"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="prev" title="支持向量机 Support Vector Machine" href="http://yuanyenogg.github.io/2019/12/06/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASupport-Vector-Machine/"><link rel="next" title="线性分类 Linear Classification" href="http://yuanyenogg.github.io/2019/11/28/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BBLinear-Classification/"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"cookieDomain":"https://blog.liming.ml/","msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"简"},
  highlight_copy: 'true',
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    title: '添加书签',
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天'

  
}</script></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#0-引论"><span class="toc-number">1.</span> <span class="toc-text">0.引论</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-预备知识"><span class="toc-number">2.</span> <span class="toc-text">1.预备知识</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-主成分分析-Principal-Component-Analysis，PCA"><span class="toc-number">3.</span> <span class="toc-text">2.主成分分析 Principal Component Analysis，PCA</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1最大投影方差角度"><span class="toc-number">3.1.</span> <span class="toc-text">2.1最大投影方差角度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2最小重构代价角度"><span class="toc-number">3.2.</span> <span class="toc-text">2.2最小重构代价角度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-奇异值分解角度-Singular-Value-Decomposition-SVD"><span class="toc-number">3.3.</span> <span class="toc-text">2.3 奇异值分解角度 Singular Value Decomposition,SVD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-4概率角度"><span class="toc-number">3.4.</span> <span class="toc-text">2.4概率角度</span></a></li></ol></li></ol></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(https://i.loli.net/2019/12/03/PhIYejAgvFkJp5d.png)"><div id="page-header"><span class="pull-left"> <a class="blog_title" id="site-name" href="/">Wildness</a></span><div class="open toggle-menu pull-right"><div class="menu-icon-first"></div><div class="menu-icon-second"></div><div class="menu-icon-third"></div></div><span class="pull-right menus"><div class="mobile_author_icon"><img class="lozad" src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'"><div class="mobile_author-info__description"></div></div><hr><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> Home</span></a><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> Archives</span></a><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> Tags</span></a><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> Categories</span></a><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> Link</span></a><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> About</span></a><script>document.body.addEventListener('touchstart', function(){ });</script></div></span><span class="pull-right"></span></div><div id="post-info"><div id="post-title"><div class="posttitle">降维 Dimensionality Reduction</div></div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 发表于 2019-12-03<span class="post-meta__separator">|</span><i class="fa fa-history" aria-hidden="true"></i> 更新于 2019-12-05</time><span class="post-meta__separator mobile_hidden">|</span><span class="mobile_hidden"><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">机器学习</a></span></div></div></div><div class="layout layout_post" id="content-inner">   <article id="post"><div class="article-container" id="post-content"><p><em>本文学习自<a href="https://space.bilibili.com/97068901/channel/detail?cid=92387" target="_blank" rel="noopener">bilibili机器学习白板推导系列</a>，<a href="https://www.visiondummy.com/2014/04/curse-dimensionality-affect-classification/" target="_blank" rel="noopener">机器学习中的维度灾难</a>。</em></p>
<h2 id="0-引论"><a href="#0-引论" class="headerlink" title="0.引论"></a>0.引论</h2><p>在线性回归的学习时，我们了解了过拟合。过拟合的产生会使我们的泛化误差很大，而我们建模的目的就是需要利用它的泛化能力。<br>为了解决过拟合的问题，我们提出了三种方式：</p>
<ul>
<li>增大数据量</li>
<li>正则化</li>
<li>降维</li>
</ul>
<p>为什么需要降维呢？这就不得不提出一个概念——<strong>维度灾难（维数灾难，Curse of Dimensionality）</strong>，过拟合是维度灾难最直接的后果之一。维度灾难通常是指在涉及到向量的计算的问题中，随着维数的增加，计算量呈指数倍增长的一种现象。而在机器学习中，如果可用的训练样本数量是固定的，那么如果增加特征维度，为了覆盖同样的特征值范围、防止过拟合，那么所需的训练样本数量就会成指数型增长。也就是说维度灾难会引起训练数据的<strong>稀疏化</strong>，使用的特征越多，数据就会变得越稀疏，从而导致分类器的分类效果就会越差。维度灾难还会造成搜索空间的数据稀疏程度分布不均。从几何角度，我们也可以用超球体和超立方体来解释维度灾难。随着维度的增加，距离函数将失去其意义，因此基于测量距离的分类起也会失去作用（具体解释见文章顶部链接）。</p>
<p>通常，降维有如下几种方法：</p>
<ul>
<li>直接降维$\rightarrow$ 特征选择（也就是直接选择部分特征）</li>
<li>线性降维$\rightarrow$ PCA</li>
<li>非线性降维$\rightarrow$ 流形</li>
</ul>
<h2 id="1-预备知识"><a href="#1-预备知识" class="headerlink" title="1.预备知识"></a>1.预备知识</h2><script type="math/tex; mode=display">
\begin{aligned}
Data:&\;X = \begin{pmatrix} x_1 & x_2 & \cdots & x_N \end{pmatrix}^{T} = \begin{pmatrix} x_1^T \\ x_2^T  \\ \vdots\ \\ x_N^T \end{pmatrix} = \begin{pmatrix} x_{11} & x_{12} & \cdots & x_{1p} \\ x_{21} & x_{22} & \cdots & x_{2p} \\ \vdots & \vdots & \ddots & \vdots \\ x_{N1} & x_{N2} & \cdots & x_{Np} \end{pmatrix}_{N*p} \\
& x_i\in \mathbb{R}^p,\,i=1,2,\cdots,N
\end{aligned}</script><p>下面介绍样本均值，样本方差的矩阵表示。</p>
<script type="math/tex; mode=display">
Sample\ Mean:\overline{x}_{p*1} = \frac {1} {N} \sum_{i=1}^Nx_i = \frac {1} {N} \begin{pmatrix}x_1 & x_2 & \cdots & x_N \end{pmatrix} \begin{pmatrix}1 \\ 1 \\ \vdots \\ 1 \end{pmatrix}_{N*1} = \frac {1} {N} X^T 1_N</script><p>其中，$ 1_N $用作简略表示，代指全为1的$N$行列向量。</p>
<script type="math/tex; mode=display">
\begin{aligned}
Sample\ Covariance:S_{p*p} =& \frac {1} {N} \sum_{i=1}^N (x_i-\overline{x})(x_i-\overline{x})^T \\
=& \frac {1} {N} \begin{pmatrix}x_1-\overline{x} & x_2-\overline{x}& \cdots & x_N-\overline{x}\end{pmatrix}  \begin{pmatrix} (x_1-\overline{x})^T \\ (x_2-\overline{x})^T \\ \vdots \\ (x_N-\overline{x})^T\end{pmatrix} \\
=& \frac {1} {N} X^T (I_N-\frac {1}{N} 1_N 1_N^T)·(I_N-\frac {1}{N} 1_N 1_N^T)^T X \\
=& \frac {1} {N}X^T H·H^T X \\
=& \frac {1} {N}X^T H X
\end{aligned}</script><p>其中，注意区分单位矩阵$I_N$和简略表示的$1_N$，$H = I_N - \frac {1}{N}1_N1_N^T $也用作简略表示，并且具有以下性质：$ H = H^T;H^2=H $。$H$也称作中心矩阵。（centering matrix）。</p>
<h2 id="2-主成分分析-Principal-Component-Analysis，PCA"><a href="#2-主成分分析-Principal-Component-Analysis，PCA" class="headerlink" title="2.主成分分析 Principal Component Analysis，PCA"></a>2.主成分分析 Principal Component Analysis，PCA</h2><p><strong>经典主成成分分析 classical PCA</strong><br>一个中心：将一组可能线性相关的变量通过正交变换，变换成一组线性入关的变量。也就是，<strong>原始特征空间的重构</strong>。<br>两个基本点：最大投影方差，最小重构距离。</p>
<h3 id="2-1最大投影方差角度"><a href="#2-1最大投影方差角度" class="headerlink" title="2.1最大投影方差角度"></a>2.1最大投影方差角度</h3><p>我们首先来探究，如何对一个向量在某个方向的投影长度进行表示。其实这个部分我们在线性分类——线性判别分析中也有所涉及。<br><img alt="向量的投影" data-src="https://pic1.superbed.cn/item/5de7aa73f1f6f81c50967a9b.png" class="lozad"><br>如上图我们要求向量$\vec{a}$在方向$\vec{b}$上的投影，我们知道向量的内积表示为$\vec{a}·\vec{b} = |a|·|b|\ cos\theta$，其中$|a|\ cos\theta$就是投影长度，那么我们就可以假设$|\vec{b}|=1$，那么这个投影长度就能用向量的内积来表示，同时我们知道内积就等于$a^Tb$或者$ab^T$。所以投影长度就可以如此表示。<br><img alt="最大投影方差" data-src="https://pic1.superbed.cn/item/5de7ab4bf1f6f81c5096aa2f.png" class="lozad"><br>如上图所示，小叉代表样本。<br>第一步，中心化。即对样本处理$x_i-\overline{x}$。假设中心化后样本分布如上图所示。<br>第二步，得到最优化问题。我们根据向量的投影表示，由于中心化后样本的均值已经为0了，那么我们可以将方差表示为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
J =& \frac{1}{N} \sum_{i=1}^N[(x_i-\overline{x})^Tu_1]^2\;s.t.\ u_1^Tu_1=1 \\
=& \frac{1}{N} \sum_{i=1}^N[u_1^T(x_i-\overline{x})(x_i-\overline{x})^Tu_1] \\
=& u_1^T \frac{1}{N} [\sum_{i=1}^N(x_i-\overline{x})(x_i-\overline{x})^T] u_1 \\
=& u_1^T S u_1
\end{aligned}</script><p>所以我们就得到了一个最优化问题：</p>
<script type="math/tex; mode=display">
\begin{cases}
\hat{u_1} = arg\ max\ u_1^TSu_1 \\
s.t.\ u_1^Tu_1=1
\end{cases}</script><p>第三步，求解最优化问题。利用<a href="https://baike.baidu.com/item/%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0%E6%B3%95/8550443?fromtitle=%E6%8B%89%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E5%AD%90%E6%B3%95&amp;fromid=1946079&amp;fr=aladdin" target="_blank" rel="noopener">拉格朗日乘子法</a>求解该最优化问题。</p>
<script type="math/tex; mode=display">
\begin{aligned} 
L(u_1,\lambda) =& u_1^TSu_1 + \lambda (1-u_1^Tu) \\
\frac{\partial{L}} {\partial{u_1}} =& 2S·u_1-\lambda·2u_1 = 0 \\
\implies & Su_1=\lambda u_1
\end{aligned}</script><p>很明显的可以看到，$u_1$就是$S$的特征向量（eigen vector），$\lambda$就是$S$的特征值（eigen value）。</p>
<h3 id="2-2最小重构代价角度"><a href="#2-2最小重构代价角度" class="headerlink" title="2.2最小重构代价角度"></a>2.2最小重构代价角度</h3><p>我们将得到的新的样本进行重构，对于一个$p$维的样本，我们得到了$u_1,u_2,\cdots,u_k$这些基向量。那么重构可以表示为：</p>
<script type="math/tex; mode=display">x_i = \sum_{k=1}^p((x_i-\overline{x})^Tu_k)\vec{u_k}+\overline{x}</script><p>如果经过降维，维数从$p$降维到了$q$。那么可以得到另一个重构的样本：</p>
<script type="math/tex; mode=display">\hat{x_i} = \sum_{k=1}^q((x_i-\overline{x})^Tu_k)\vec{u_k}+\overline{x}\  ,q<p</script><p>那么重构代价函数可以表示为：</p>
<script type="math/tex; mode=display">
\begin{aligned}
J =& \frac {1} {N} \sum_{i=1}^N \| x_i - \hat{x_i}\|^2 \\
=& \frac {1} {N} \sum_{i=1}^N \| \sum_{k=q+1}^p((x_i-\overline{x})^Tu_k)\vec{u_k}\|^2 \\
=& \frac {1} {N} \sum_{i=1}^N \sum_{k=q+1}^p((x_i-\overline{x})^Tu_k))^2 \\
=& \sum_{k=q+1}^p \frac {1} {N} \sum_{i=1}^N ((x_i-\overline{x})^Tu_k))^2 \\
=& \sum_{k=q+1}^p u_k^TSu_k \\
s.t.\ u_k^Tu_k=1
\end{aligned}</script><p>由此可以得到一个最小化的优化问题：</p>
<script type="math/tex; mode=display">
\begin{cases}
\hat{u_k} = arg\ min\ \sum_{k=p+1}^q u_k^TSu_k \\
s.t.\ u_k^Tu_k=1
\end{cases}</script><p>相同的解法，如果说上面最大化投影方差是得到了一组特征值，选择最大的$q$个，那么从这个角度，这个最小化问题的解就是最小的几个特征值，其实是上一个问题的余项。可以把这个最小化问题看成$(p-q)$个最小化问题，具体求解就省略了。</p>
<p>因此，从两个不同的角度我们可以得到相同的答案。</p>
<h3 id="2-3-奇异值分解角度-Singular-Value-Decomposition-SVD"><a href="#2-3-奇异值分解角度-Singular-Value-Decomposition-SVD" class="headerlink" title="2.3 奇异值分解角度 Singular Value Decomposition,SVD"></a>2.3 奇异值分解角度 Singular Value Decomposition,SVD</h3><p>我们把上面两个角度可以看作是对样本协方差矩阵$S$的特征值分解（Eigen Value Decomposition,EVD）：</p>
<script type="math/tex; mode=display">S = GKG^T;G^TG=I;K=\begin{pmatrix}k_1 & & & \\ & k_2 & & \\ & & \ddots & \\ & & & k_p \end{pmatrix},k_1 \geq k_2 \geq \cdots \geq k_p</script><p>下面我们从奇异值分析的角度来看这个问题。<br>首先，对样本中心化后的矩阵$HX$做奇异值分解：</p>
<script type="math/tex; mode=display">HX = U\Sigma V^T;U^TU=I;V^TV =VV^T=I;\Sigma是对角矩阵</script><p>接着，我们对协方差矩阵缩放，省略前面的$\frac {1} {N}$方便分析。那么协方差矩阵可以表示为：</p>
<script type="math/tex; mode=display">S_{p*p} = X^THX=X^TH^THX=V\Sigma U^TU \Sigma V^T = V\Sigma^2V^T</script><p>这个式子与上面做特征值分解的式子可以相对应，那么也就有了$G=V,K=\Sigma^2$。<br>然后，我们再定义一个矩阵$T$：</p>
<script type="math/tex; mode=display">T_{N*N} = HX X^TH^T = U\Sigma V^T V \Sigma U^T = U \Sigma^2U^T</script><p>不难看出$T$和$S$具有相同的eigen value。<br>我们知道对$S$矩阵做特征分解可以得到方向（主成分），然后用$HX·V$就可以得到在坐标矩阵，而$HX·V = U\Sigma V^T V = U\Sigma$。<br>也就是说对$T$矩阵做特征分解可以直接得到坐标。我们将这种对$T$矩阵做特征分解的方法称为<strong>主坐标分析（Principle Coordinate Analysis,PCoA）</strong>。</p>
<h3 id="2-4概率角度"><a href="#2-4概率角度" class="headerlink" title="2.4概率角度"></a>2.4概率角度</h3><p>这一节从概率角度理解主成分分析，从概率角度可以称作<strong>P-PCA</strong>。我们知道降维的目的是让原始数据从$x (observed\ data)$所在的$p$维空间对应到$z(latent\ data)$所在的$q$维空间。即：</p>
<script type="math/tex; mode=display">x\in \mathbb{R}^p \rightarrow z \in \mathbb{R}^q,q<p</script><p>那么我们可以给$z$一个先验，对$x$做线性变换再加上一个噪声，并且噪声是满足一个分布。于是我们就得到了这样一组假设：</p>
<script type="math/tex; mode=display">
\begin{cases}
z \sim N(0_q,I_q) \\
x = w_{p*q}z+\mu+\varepsilon \\
\varepsilon \sim N(0,\sigma^2I_p)
\end{cases}</script><p>这样一组假设得到的也是一个线性高斯模型（Linear Gaussion Model）。<br>P-PCA主要有两个部分，一个部分是inference也就是求 $P(z|x)$，另一个是learning也就是求这些参数$w,\mu ,\sigma^2$。参数需要用EM算法求出，这里先不求解，我们主要是求P-PCA模型的推断。<br>为了求$z|x$的分布，首先我们已经有了$z$的分布，下一步求$x|z$的分布。在求<br>$x|z$的分布可以将$z$看作是一个常数来处理，那么有：</p>
<script type="math/tex; mode=display">
\begin{aligned}
E[x|z] = & E[wz+\mu+\varepsilon] = wz+\mu \\
Var[x|z]= & Var[wz+\mu+\varepsilon] = \sigma^2I \\
\implies x|z \sim & N(wz+\mu,\sigma^2I)
\end{aligned}</script><p>然后在此基础上可以求$x$的分布，可以得到：</p>
<script type="math/tex; mode=display">
\begin{aligned}
E[x] = & E[wz+\mu+\varepsilon] =E[wz+\mu] +E[\varepsilon] = \mu \\
Var[x]= & Var[wz+\mu+\varepsilon] = Var[wz]+Var[\varepsilon] = wIw^T+\sigma^2I  = ww^T+\sigma^2I\\
\implies x \sim & N(\mu,ww^T+\sigma^2I)
\end{aligned}</script><p>下面求$P(z|x)$。在这里先讲一个引论：</p>
<blockquote>
<p><strong>如何从联合概率分布求条件概率分布？</strong><br>假设<script type="math/tex">x=\begin{pmatrix}x_a \\ x_b \end{pmatrix} , x\sim (\begin{bmatrix} \mu_a \\ \mu_b \end{bmatrix},\begin{bmatrix} \Sigma_{aa} & \Sigma_{ab} \\ \Sigma_{ba} & \Sigma_{bb} \end{bmatrix})</script></p>
<script type="math/tex; mode=display">\begin{aligned} x_{ba} =& x_b- \Sigma_{ba}\Sigma_{aa}^{-1}x_a \\ \mu_{ba}=&\mu_b - \Sigma_{ba}\Sigma_{aa}^{-1}\mu_a \\ \Sigma_{bba} =& \Sigma_{bb}-\Sigma_{ba}\Sigma_{aa}^{-1}\Sigma_{ab} \\ x_{ba} \sim& N(\mu_{ba},\Sigma_{bba})  \end{aligned}</script><p>又因为<script type="math/tex">x_b = x_{ba}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a</script><br>所以可以得到：</p>
<script type="math/tex; mode=display">\begin{cases} E[x_b|x_a]= E[x_{ba}]+\Sigma_{ba}\Sigma_{aa}^{-1}x_a=\mu_{ba}+\Sigma_{ba}\Sigma_{aa}^{-1}x_a=\mu_b+\Sigma_{ba}\Sigma_{aa}^{-1}(x_a-\mu_a) \\ Var[x_b|x_a]=Var[x_{ba}]=\Sigma_{bba} \end{cases}</script><script type="math/tex; mode=display">\implies x_b|x_a \sim N(\mu_b+\Sigma_{ba}\Sigma_{aa}^{-1}(x_a-\mu_a),\Sigma_{bba})</script></blockquote>
<p>根据这个引论，只需求出$x$和$z$的联合概率分布即可：</p>
<script type="math/tex; mode=display">\begin{pmatrix}x \\ z \end{pmatrix} \sim N(\begin{bmatrix} \mu \\ 0 \end{bmatrix},\begin{bmatrix} ww^T+\sigma^2I & \Delta \\ \Delta^T & I \end{bmatrix})</script><p>求联合概率分布需要求出$\Delta$：</p>
<script type="math/tex; mode=display">\begin{aligned} \Delta = Cov(x,z) = E[(x-\mu)(z-0)^T]= & E[(wz+\varepsilon )z^T]\\ =&E[wzz^T+\varepsilon z^T] \\=& wE[zz^T]+E[\varepsilon]E[z^T] \\ =& wI+0 \\ = &w\end{aligned}</script><p>那么根据公式就可以求出$P(z|x)$。</p>
<p>至此，P-PCA模型的推断就推导完毕了。降维这一节也结束了。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined" target="_blank" rel="noopener">Yuan Ye</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://yuanyenogg.github.io/2019/12/03/%E9%99%8D%E7%BB%B4/">http://yuanyenogg.github.io/2019/12/03/%E9%99%8D%E7%BB%B4/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank" rel="noopener">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://yuanyenogg.github.io">Wildness</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E7%AC%94%E8%AE%B0/">笔记    </a><a class="post-meta__tags" href="/tags/%E7%AE%97%E6%B3%95/">算法    </a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2019/12/03/PhIYejAgvFkJp5d.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script></div></div><nav class="pagination_post" id="pagination"><div class="prev-post pull-left"><a href="/2019/12/06/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASupport-Vector-Machine/"><img class="prev_cover lozad" data-src="https://i.loli.net/2019/12/06/eYFESK4NuXCcvQ8.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">上一篇</div><div class="prev_info"><span>支持向量机 Support Vector Machine</span></div></a></div><div class="next-post pull-right"><a href="/2019/11/28/%E7%BA%BF%E6%80%A7%E5%88%86%E7%B1%BBLinear-Classification/"><img class="next_cover lozad" data-src="https://i.loli.net/2019/11/28/d9fj75LGWyKelOg.png" onerror="onerror=null;src='/img/404.jpg'"><div class="label">下一篇</div><div class="next_info"><span>线性分类 Linear Classification</span></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fa fa-fw fa-thumbs-up" aria-hidden="true"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2019/12/06/支持向量机Support-Vector-Machine/" title="支持向量机 Support Vector Machine"><img class="relatedPosts_cover lozad"data-src="https://i.loli.net/2019/12/06/eYFESK4NuXCcvQ8.png"><div class="relatedPosts_title">支持向量机 Support Vector Machine</div></a></div><div class="relatedPosts_item"><a href="/2019/11/28/线性分类Linear-Classification/" title="线性分类 Linear Classification"><img class="relatedPosts_cover lozad"data-src="https://i.loli.net/2019/11/28/d9fj75LGWyKelOg.png"><div class="relatedPosts_title">线性分类 Linear Classification</div></a></div><div class="relatedPosts_item"><a href="/2019/11/20/线性回归Linear-Regression/" title="线性回归 Linear Regression"><img class="relatedPosts_cover lozad"data-src="https://i.loli.net/2019/11/20/wt8avdzIUC5iM4Z.png"><div class="relatedPosts_title">线性回归 Linear Regression</div></a></div><div class="relatedPosts_item"><a href="/2020/03/19/MIT-6-824/" title="MIT6.824分布式系统"><img class="relatedPosts_cover lozad"data-src="undefined"><div class="relatedPosts_title">MIT6.824分布式系统</div></a></div><div class="relatedPosts_item"><a href="/2019/12/30/二叉树遍历/" title="二叉树遍历——递归，栈，莫里斯树"><img class="relatedPosts_cover lozad"data-src="https://i.loli.net/2019/12/30/Mi6lZBGjX1wmE5N.png"><div class="relatedPosts_title">二叉树遍历——递归，栈，莫里斯树</div></a></div><div class="relatedPosts_item"><a href="/2019/11/27/深入理解计算机系统/" title="深入理解计算机系统"><img class="relatedPosts_cover lozad"data-src="https://i.loli.net/2019/11/28/RjFSQsG1rkV6eDo.png"><div class="relatedPosts_title">深入理解计算机系统</div></a></div></div><div class="clear_both"></div></div></div></div><footer><div class="layout" id="footer"><div class="copyright">&copy;2019 - 2020 By Yuan Ye</div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><section class="rightside" id="rightside"><i class="fa fa-book" id="readmode" title="阅读模式"> </i><i class="fa fa-plus" id="font_plus" title="放大字体"></i><i class="fa fa-minus" id="font_minus" title="缩小字体"></i><a class="translate_chn_to_cht" id="translateLink" href="javascript:translatePage();" target="_blank" rel="noopener" title="简繁转换">简</a><i class="fa fa-moon-o nightshift" id="nightshift" title="夜间模式"></i></section><div id="post_bottom"><div id="post_bottom_items"><a id="to_comment" href="#post-comment"><i class="scroll_to_comment fa fa-comments"></i></a><i class="fa fa-list" id="mobile_toc"></i><div id="toc_mobile"><div class="toc_mobile_headline">目录</div><ol class="toc_mobile_items"><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#0-引论"><span class="toc_mobile_items-number">1.</span> <span class="toc_mobile_items-text">0.引论</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#1-预备知识"><span class="toc_mobile_items-number">2.</span> <span class="toc_mobile_items-text">1.预备知识</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-2"><a class="toc_mobile_items-link" href="#2-主成分分析-Principal-Component-Analysis，PCA"><span class="toc_mobile_items-number">3.</span> <span class="toc_mobile_items-text">2.主成分分析 Principal Component Analysis，PCA</span></a><ol class="toc_mobile_items-child"><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2-1最大投影方差角度"><span class="toc_mobile_items-number">3.1.</span> <span class="toc_mobile_items-text">2.1最大投影方差角度</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2-2最小重构代价角度"><span class="toc_mobile_items-number">3.2.</span> <span class="toc_mobile_items-text">2.2最小重构代价角度</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2-3-奇异值分解角度-Singular-Value-Decomposition-SVD"><span class="toc_mobile_items-number">3.3.</span> <span class="toc_mobile_items-text">2.3 奇异值分解角度 Singular Value Decomposition,SVD</span></a></li><li class="toc_mobile_items-item toc_mobile_items-level-3"><a class="toc_mobile_items-link" href="#2-4概率角度"><span class="toc_mobile_items-number">3.4.</span> <span class="toc_mobile_items-text">2.4概率角度</span></a></li></ol></li></ol></div></div></div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/js-cookie@2/src/js.cookie.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="/js/nightshift.js"></script><script src="/js/tw_cn.js"></script><script>translateInitilization()

</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page@1.2.2/instantpage.min.js" type="module"></script><script src="https://cdn.jsdelivr.net/npm/lozad/dist/lozad.min.js"></script><script>const observer = lozad(); // lazy loads elements with default selector as '.lozad'
observer.observe();</script></body></html>